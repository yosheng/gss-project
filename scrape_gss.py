import requests
import os
import math
import re
import json
import glob
import argparse
from dotenv import load_dotenv
from supabase import create_client, Client

# --- å¸¸æ•¸èˆ‡é…ç½® ---
# è¼‰å…¥ .env æª”æ¡ˆä¸­çš„ç’°å¢ƒè®Šæ•¸
load_dotenv()

# Supabase & API é…ç½®
SUPABASE_URL = os.getenv("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.getenv("NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY")
API_AUTH_TOKEN = os.getenv("API_AUTH_TOKEN")
API_URL = 'https://assistant.gss.com.tw/QuickSearchApi/index/extendrequest/index/SearchEmployee'
TABLE_NAME = 'gss_employees'
DATA_DIR = 'data'  # ç”¨æ–¼å­˜æ”¾ JSON æª”æ¡ˆçš„è³‡æ–™å¤¾

# è¤‡è£½ cURL çš„ Headers
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0',
    'Accept': 'application/json, text/plain, */*',
    'Content-Type': 'application/json',
    'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6,zh-TW;q=0.5',
    'authorization': API_AUTH_TOKEN,
    'origin': 'https://assistant.gss.com.tw',
    'priority': 'u=1, i',
    'sec-ch-ua': '"Not;A=Brand";v="99", "Microsoft Edge";v="139", "Chromium";v="139"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    # Cookie å¯ä»¥è¦–æƒ…æ³åŠ å…¥ï¼Œä½†é€šå¸¸ Authorization Token å·²è¶³å¤ 
    # 'Cookie': '...'
}

# è«‹æ±‚çš„ Body (Payload)
BASE_PAYLOAD = {
  "isWork": True,
  "isDeparture": False,
  "dept": "",
  "employee": "",
  "ofcExt": "",
  "pageIndex": 0,
  "subordinates": [
    "YOSHENG_ZHANG"
  ]
}


# --- è¼”åŠ©å‡½æ•¸ ---
def camel_to_snake(name):
    """å°‡é§å³°å¼å‘½å (camelCase) è½‰æ›ç‚ºè›‡å½¢å¼å‘½å (snake_case)"""
    # ã€ä¿®æ­£ã€‘åœ¨ raw string ä¸­ï¼Œç”¨æ–¼ç¾¤çµ„å¼•ç”¨çš„åæ–œç·šæ‡‰ç‚ºå–®ä¸€åæ–œç·š
    s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
    return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()


# --- æ ¸å¿ƒåŠŸèƒ½ ---
def fetch_and_save_from_api():
    """å¾ API çˆ¬å–æ‰€æœ‰å“¡å·¥è³‡æ–™ï¼Œä¸¦å°‡æ¯é çµæœå­˜ç‚º JSON æª”æ¡ˆã€‚"""
    # ç¢ºä¿ data è³‡æ–™å¤¾å­˜åœ¨
    os.makedirs(DATA_DIR, exist_ok=True)

    all_employees = []
    page_index = 0
    total_records = -1

    while True:
        payload = BASE_PAYLOAD.copy()
        payload['pageIndex'] = page_index

        print(f"ğŸš€ æ­£åœ¨å¾ API çˆ¬å–ç¬¬ {page_index + 1} é çš„è³‡æ–™...")

        try:
            response = requests.post(API_URL, headers=HEADERS, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()

            # å¯«å…¥ JSON æª”æ¡ˆ
            file_path = os.path.join(DATA_DIR, f"page_{page_index}.json")
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            print(f"ğŸ’¾ å·²å°‡çµæœå„²å­˜è‡³ {file_path}")

            if data.get('status') != 200:
                print(f"âŒ API å›æ‡‰éŒ¯èª¤: {data.get('message')}")
                break

            rows = data.get('data', {}).get('rows', [])
            if not rows:
                print("âœ… API å·²æ²’æœ‰æ›´å¤šè³‡æ–™ï¼Œçˆ¬å–å®Œæˆã€‚")
                break

            if total_records == -1:
                total_records = data.get('data', {}).get('total', 0)
                print(f"ğŸ” ç™¼ç¾ç¸½å…±æœ‰ {total_records} ç­†å“¡å·¥è³‡æ–™ã€‚")

            all_employees.extend(rows)
            page_index += 1

            if len(all_employees) >= total_records:
                print("âœ… å·²çˆ¬å–æ‰€æœ‰å“¡å·¥è³‡æ–™ã€‚")
                break
        except requests.exceptions.RequestException as e:
            print(f"âŒ ç¶²è·¯è«‹æ±‚å¤±æ•—: {e}")
            return None

    return all_employees


def load_from_local():
    """å¾æœ¬åœ° data è³‡æ–™å¤¾è®€å–æ‰€æœ‰ JSON æª”æ¡ˆä¸¦åˆä½µè³‡æ–™ã€‚"""
    print("ğŸ“‚ æ­£åœ¨å¾æœ¬åœ° `data` è³‡æ–™å¤¾è®€å–è³‡æ–™...")
    if not os.path.isdir(DATA_DIR):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° `{DATA_DIR}` è³‡æ–™å¤¾ã€‚è«‹å…ˆåŸ·è¡Œ `--source api` ä¾†çˆ¬å–ä¸¦å„²å­˜è³‡æ–™ã€‚")
        return None

    all_employees = []
    json_files = sorted(glob.glob(os.path.join(DATA_DIR, 'page_*.json')),
                        key=lambda x: int(re.search(r'page_(\d+).json', x).group(1)))

    if not json_files:
        print(f"âš ï¸ åœ¨ `{DATA_DIR}` ä¸­æ‰¾ä¸åˆ°ä»»ä½• `page_*.json` æª”æ¡ˆã€‚")
        return []

    for file_path in json_files:
        print(f"   - æ­£åœ¨è®€å– {os.path.basename(file_path)}...")
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            rows = data.get('data', {}).get('rows', [])
            all_employees.extend(rows)

    print(f"âœ… å¾æœ¬åœ°æª”æ¡ˆæˆåŠŸè¼‰å…¥ {len(all_employees)} ç­†è³‡æ–™ã€‚")
    return all_employees


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    parser = argparse.ArgumentParser(description="çˆ¬å– GSS å“¡å·¥è³‡æ–™ä¸¦å­˜å…¥ Supabaseã€‚")
    parser.add_argument('--source', type=str, choices=['api', 'local'], default='api',
                        help="é¸æ“‡è³‡æ–™ä¾†æºï¼š'api' (å¾ç¶²è·¯çˆ¬å–) æˆ– 'local' (å¾æœ¬åœ° data è³‡æ–™å¤¾è®€å–)ã€‚é è¨­ç‚º 'api'ã€‚")
    args = parser.parse_args()

    # --- æ­¥é©Ÿ 1: æ ¹æ“šä¾†æºç²å–è³‡æ–™ ---
    if args.source == 'api':
        employees_data = fetch_and_save_from_api()
    else:  # args.source == 'local'
        employees_data = load_from_local()

    if not employees_data:
        print("æœªèƒ½ç²å–ä»»ä½•å“¡å·¥è³‡æ–™ï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
        return

    # --- æ­¥é©Ÿ 2: è½‰æ›è³‡æ–™æ ¼å¼ ---

    # å®šç¾©è³‡æ–™è¡¨ä¸­çš„æœ‰æ•ˆæ¬„ä½
    VALID_COLUMNS = {
        'emp_id',
        'c_name',
        'e_name',
        'dep_code',
        'job_status',
        'encrypt_emp_id',
        'per_seril_no',
        'encrypt_per_seril_no',
        'tit_name',
        'dep_name_act',
        'ofc_ext',
        'introduction',
        'cmp_ent_dte',
        'lev_exp_sdate',
        'user_id',
        'is_show_private_data',
        'photo_type',
        'is_show_download_photo',
        'cmp_code',
        'created_at'
    }

    transformed_data = []
    for record in employees_data:
        # å…ˆè½‰æ›æ‰€æœ‰éµç‚º snake_case
        snake_case_record = {camel_to_snake(key): value for key, value in record.items()}

        # åªä¿ç•™è³‡æ–™è¡¨ä¸­å­˜åœ¨çš„æ¬„ä½
        filtered_record = {
            k: (v if v != "" else None)
            for k, v in snake_case_record.items()
            if k in VALID_COLUMNS
        }

        # ç‰¹æ®Šè™•ç†æ—¥æœŸæ¬„ä½
        if 'cmp_ent_dte' in filtered_record and filtered_record['cmp_ent_dte']:
            try:
                # å‡è¨­æ—¥æœŸæ ¼å¼ç‚º "YYYY-MM-DD"
                filtered_record['cmp_ent_dte'] = filtered_record['cmp_ent_dte'].split('T')[0]
            except:
                filtered_record['cmp_ent_dte'] = None

        transformed_data.append(filtered_record)

    print("ğŸ”„ è³‡æ–™æ ¼å¼è½‰æ›å®Œæˆ (camelCase -> snake_case)ã€‚")

    # --- æ­¥é©Ÿ 3: åˆå§‹åŒ– Supabase ä¸¦å¯«å…¥è³‡æ–™ ---
    if not all([SUPABASE_URL, SUPABASE_KEY]):
        print("ğŸ”´ éŒ¯èª¤ï¼šè«‹æª¢æŸ¥ .env æª”æ¡ˆä¸­çš„ Supabase URL/KEY æ˜¯å¦å·²è¨­å®šã€‚")
        return

    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        print("âœ… Supabase å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
    except Exception as e:
        print(f"ğŸ”´ Supabase åˆå§‹åŒ–å¤±æ•—: {e}")
        return

    print(f"æ­£åœ¨å°‡ {len(transformed_data)} ç­†è³‡æ–™å¯«å…¥ Supabase çš„ '{TABLE_NAME}' è³‡æ–™è¡¨ä¸­...")
    try:
        response = supabase.table(TABLE_NAME).upsert(
            transformed_data,
            on_conflict='emp_id'
        ).execute()

        if response.data:
            print(f"ğŸ‰ æˆåŠŸï¼è³‡æ–™å·²å¯«å…¥ Supabaseã€‚")
        else:
            print(f"âš ï¸ æ“ä½œå®Œæˆï¼Œä½† Supabase æœªè¿”å›æˆåŠŸè³‡æ–™ã€‚è«‹æª¢æŸ¥è³‡æ–™è¡¨ã€‚")
            if hasattr(response, 'error') and response.error:
                print(f"   éŒ¯èª¤è©³æƒ…: {response.error}")

    except Exception as e:
        print(f"ğŸ”´ å¯«å…¥ Supabase æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


if __name__ == "__main__":
    main()