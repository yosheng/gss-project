import requests
import os
import math
import re
import json
import glob
import argparse
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client

# --- å¸¸æ•¸èˆ‡é…ç½® ---
# è¼‰å…¥ .env æª”æ¡ˆä¸­çš„ç’°å¢ƒè®Šæ•¸
load_dotenv()

# Supabase & API é…ç½®
SUPABASE_URL = os.getenv("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.getenv("NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY")
API_AUTH_TOKEN = os.getenv("API_AUTH_TOKEN")
API_URL = 'https://assistant.gss.com.tw/QuickSearchApi/index/extendrequest/index/SearchEmployee'
TABLE_NAME = 'gss_employees'
DATA_DIR = 'data'  # ç”¨æ–¼å­˜æ”¾ JSON æª”æ¡ˆçš„è³‡æ–™å¤¾

# è¤‡è£½ cURL çš„ Headers
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0',
    'Accept': 'application/json, text/plain, */*',
    'Content-Type': 'application/json',
    'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6,zh-TW;q=0.5',
    'authorization': API_AUTH_TOKEN,
    'origin': 'https://assistant.gss.com.tw',
    'priority': 'u=1, i',
    'sec-ch-ua': '"Not;A=Brand";v="99", "Microsoft Edge";v="139", "Chromium";v="139"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    # Cookie å¯ä»¥è¦–æƒ…æ³åŠ å…¥ï¼Œä½†é€šå¸¸ Authorization Token å·²è¶³å¤ 
    # 'Cookie': '...'
}

# è«‹æ±‚çš„ Body (Payload)
BASE_PAYLOAD = {
  "isWork": True,
  "isDeparture": False,
  "dept": "",
  "employee": "",
  "ofcExt": "",
  "pageIndex": 0,
  "subordinates": [
    "YOSHENG_ZHANG"
  ]
}


# --- è¼”åŠ©å‡½æ•¸ ---
def camel_to_snake(name):
    """å°‡é§å³°å¼å‘½å (camelCase) è½‰æ›ç‚ºè›‡å½¢å¼å‘½å (snake_case)"""
    # ã€ä¿®æ­£ã€‘åœ¨ raw string ä¸­ï¼Œç”¨æ–¼ç¾¤çµ„å¼•ç”¨çš„åæ–œç·šæ‡‰ç‚ºå–®ä¸€åæ–œç·š
    s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
    return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()


# --- æ ¸å¿ƒåŠŸèƒ½ ---
def fetch_and_save_from_api():
    """å¾ API çˆ¬å–æ‰€æœ‰å“¡å·¥è³‡æ–™ï¼Œä¸¦å°‡æ¯é çµæœå­˜ç‚º JSON æª”æ¡ˆã€‚"""
    # ç¢ºä¿ data è³‡æ–™å¤¾å­˜åœ¨
    os.makedirs(DATA_DIR, exist_ok=True)

    all_employees = []
    page_index = 0
    total_records = -1

    while True:
        payload = BASE_PAYLOAD.copy()
        payload['pageIndex'] = page_index

        print(f"ğŸš€ æ­£åœ¨å¾ API çˆ¬å–ç¬¬ {page_index + 1} é çš„è³‡æ–™...")

        try:
            response = requests.post(API_URL, headers=HEADERS, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()

            # å¯«å…¥ JSON æª”æ¡ˆ
            file_path = os.path.join(DATA_DIR, f"page_{page_index}.json")
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            print(f"ğŸ’¾ å·²å°‡çµæœå„²å­˜è‡³ {file_path}")

            if data.get('status') != 200:
                print(f"âŒ API å›æ‡‰éŒ¯èª¤: {data.get('message')}")
                break

            rows = data.get('data', {}).get('rows', [])
            if not rows:
                print("âœ… API å·²æ²’æœ‰æ›´å¤šè³‡æ–™ï¼Œçˆ¬å–å®Œæˆã€‚")
                break

            if total_records == -1:
                total_records = data.get('data', {}).get('total', 0)
                print(f"ğŸ” ç™¼ç¾ç¸½å…±æœ‰ {total_records} ç­†å“¡å·¥è³‡æ–™ã€‚")

            all_employees.extend(rows)
            page_index += 1

            if len(all_employees) >= total_records:
                print("âœ… å·²çˆ¬å–æ‰€æœ‰å“¡å·¥è³‡æ–™ã€‚")
                break
        except requests.exceptions.RequestException as e:
            print(f"âŒ ç¶²è·¯è«‹æ±‚å¤±æ•—: {e}")
            return None

    return all_employees


def load_from_local():
    """å¾æœ¬åœ° data è³‡æ–™å¤¾è®€å–æ‰€æœ‰ JSON æª”æ¡ˆä¸¦åˆä½µè³‡æ–™ã€‚"""
    print("ğŸ“‚ æ­£åœ¨å¾æœ¬åœ° `data` è³‡æ–™å¤¾è®€å–è³‡æ–™...")
    if not os.path.isdir(DATA_DIR):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° `{DATA_DIR}` è³‡æ–™å¤¾ã€‚è«‹å…ˆåŸ·è¡Œ `--source api` ä¾†çˆ¬å–ä¸¦å„²å­˜è³‡æ–™ã€‚")
        return None

    all_employees = []
    json_files = sorted(glob.glob(os.path.join(DATA_DIR, 'page_*.json')),
                        key=lambda x: int(re.search(r'page_(\d+).json', x).group(1)))

    if not json_files:
        print(f"âš ï¸ åœ¨ `{DATA_DIR}` ä¸­æ‰¾ä¸åˆ°ä»»ä½• `page_*.json` æª”æ¡ˆã€‚")
        return []

    for file_path in json_files:
        print(f"   - æ­£åœ¨è®€å– {os.path.basename(file_path)}...")
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            rows = data.get('data', {}).get('rows', [])
            all_employees.extend(rows)

    print(f"âœ… å¾æœ¬åœ°æª”æ¡ˆæˆåŠŸè¼‰å…¥ {len(all_employees)} ç­†è³‡æ–™ã€‚")
    return all_employees


def fetch_existing_employees_from_supabase(supabase: Client):
    """
    å¾ Supabase æ‹‰å–æ‰€æœ‰ç¾æœ‰å“¡å·¥è³‡æ–™ã€‚

    Args:
        supabase: Supabase å®¢æˆ¶ç«¯å¯¦ä¾‹

    Returns:
        set: emp_id çš„é›†åˆï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥è©¢
    """
    print("ğŸ” æ­£åœ¨å¾ Supabase æ‹‰å–ç¾æœ‰å“¡å·¥è³‡æ–™...")

    try:
        # æ‹‰å–æ‰€æœ‰å“¡å·¥è³‡æ–™ï¼ˆemp_id ç”¨æ–¼æ¯”å°ï¼‰
        response = supabase.table(TABLE_NAME).select("emp_id").execute()

        if not response.data:
            print("â„¹ï¸  Supabase ä¸­ç›®å‰æ²’æœ‰ä»»ä½•å“¡å·¥è³‡æ–™ã€‚")
            return set()

        # å°‡è³‡æ–™è½‰æ›ç‚º emp_id çš„é›†åˆï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥è©¢
        existing_emp_ids = {emp['emp_id'] for emp in response.data}
        print(f"âœ… æˆåŠŸæ‹‰å– {len(existing_emp_ids)} ç­†ç¾æœ‰å“¡å·¥è³‡æ–™ã€‚")

        return existing_emp_ids

    except Exception as e:
        print(f"âŒ å¾ Supabase æ‹‰å–è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return set()


def sync_employees_to_supabase(supabase: Client, transformed_data: list, departed_status='é›¢è·'):
    """
    åŒæ­¥å“¡å·¥è³‡æ–™åˆ° Supabaseï¼Œå¯¦ç¾å¢é‡æ›´æ–°å’Œé›¢è·æ¨™è¨˜ã€‚

    åŒæ­¥é‚è¼¯ï¼š
    1. API è¿”å›çš„å“¡å·¥åœ¨ Supabase ä¸å­˜åœ¨ â†’ æ–°å¢è¨˜éŒ„
    2. API è¿”å›çš„å“¡å·¥åœ¨ Supabase å·²å­˜åœ¨ â†’ æ›´æ–°è¨˜éŒ„ï¼ˆåŒ…æ‹¬ last_updated_atï¼‰
    3. Supabase å­˜åœ¨ä½† API æœªè¿”å›çš„å“¡å·¥ â†’ æ¨™è¨˜ç‚ºé›¢è·ï¼ˆæ›´æ–° job_statusï¼‰

    Args:
        supabase: Supabase å®¢æˆ¶ç«¯å¯¦ä¾‹
        transformed_data: è½‰æ›å¾Œçš„å“¡å·¥è³‡æ–™åˆ—è¡¨
        departed_status: é›¢è·ç‹€æ…‹çš„å€¼ï¼ˆé è¨­ç‚º 'é›¢è·'ï¼‰

    Returns:
        dict: åŒ…å«çµ±è¨ˆè³‡è¨Šçš„å­—å…¸ {'new': æ–°å¢æ•¸, 'updated': æ›´æ–°æ•¸, 'departed': é›¢è·æ•¸}
    """
    print("\n" + "="*50)
    print("é–‹å§‹åŒæ­¥è³‡æ–™åˆ° Supabase...")
    print("="*50)

    # çµ±è¨ˆè³‡è¨Š
    stats = {'new': 0, 'updated': 0, 'departed': 0}

    # 1. æ‹‰å– Supabase ä¸­ç¾æœ‰çš„å“¡å·¥ emp_id
    existing_emp_ids = fetch_existing_employees_from_supabase(supabase)

    # 2. å¾ API æ•¸æ“šä¸­æå– emp_id é›†åˆ
    api_emp_ids = {record['emp_id'] for record in transformed_data if 'emp_id' in record}
    print(f"\nğŸ“Š å¾ API ç²å–äº† {len(api_emp_ids)} ç­†å“¡å·¥è³‡æ–™ã€‚")

    # 3. åˆ†æéœ€è¦æ–°å¢å’Œæ›´æ–°çš„å“¡å·¥
    new_emp_ids = api_emp_ids - existing_emp_ids
    update_emp_ids = api_emp_ids & existing_emp_ids

    print(f"   - éœ€è¦æ–°å¢ï¼š{len(new_emp_ids)} ç­†")
    print(f"   - éœ€è¦æ›´æ–°ï¼š{len(update_emp_ids)} ç­†")

    # 4. æ–°å¢æˆ–æ›´æ–°å“¡å·¥è³‡æ–™
    current_time = datetime.now().isoformat()

    for record in transformed_data:
        emp_id = record.get('emp_id')
        if not emp_id:
            continue

        # æ·»åŠ  last_updated_at å­—æ®µ
        record['last_updated_at'] = current_time

    # ä½¿ç”¨ upsert ä¸€æ¬¡æ€§è™•ç†æ–°å¢å’Œæ›´æ–°
    try:
        if transformed_data:
            print(f"\nğŸ”„ æ­£åœ¨åŸ·è¡Œ upsert æ“ä½œ...")
            response = supabase.table(TABLE_NAME).upsert(
                transformed_data,
                on_conflict='emp_id'
            ).execute()

            if response.data:
                stats['new'] = len(new_emp_ids)
                stats['updated'] = len(update_emp_ids)
                print(f"âœ… æˆåŠŸæ–°å¢/æ›´æ–° {len(response.data)} ç­†è³‡æ–™ã€‚")
            else:
                print(f"âš ï¸  Upsert æ“ä½œå®Œæˆï¼Œä½†æœªè¿”å›è³‡æ–™ã€‚")

    except Exception as e:
        print(f"âŒ Upsert æ“ä½œå¤±æ•—: {e}")
        return stats

    # 5. æ¨™è¨˜é›¢è·çš„å“¡å·¥
    departed_emp_ids = existing_emp_ids - api_emp_ids

    if departed_emp_ids:
        print(f"\nğŸ‘‹ ç™¼ç¾ {len(departed_emp_ids)} ä½å“¡å·¥å·²é›¢è·ï¼Œæ­£åœ¨æ›´æ–°ç‹€æ…‹...")

        for emp_id in departed_emp_ids:
            try:
                response = supabase.table(TABLE_NAME).update({
                    'job_status': departed_status,
                    'last_updated_at': current_time
                }).eq('emp_id', emp_id).execute()

                if response.data:
                    stats['departed'] += 1
                    print(f"   - å·²æ¨™è¨˜ {emp_id} ç‚ºé›¢è·")

            except Exception as e:
                print(f"   âŒ æ›´æ–° {emp_id} é›¢è·ç‹€æ…‹å¤±æ•—: {e}")

        print(f"âœ… æˆåŠŸæ¨™è¨˜ {stats['departed']} ä½å“¡å·¥ç‚ºé›¢è·ã€‚")
    else:
        print(f"\nâ„¹ï¸  æ²’æœ‰å“¡å·¥é›¢è·ã€‚")

    return stats


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    parser = argparse.ArgumentParser(description="çˆ¬å– GSS å“¡å·¥è³‡æ–™ä¸¦å­˜å…¥ Supabaseã€‚")
    parser.add_argument('--source', type=str, choices=['api', 'local'], default='api',
                        help="é¸æ“‡è³‡æ–™ä¾†æºï¼š'api' (å¾ç¶²è·¯çˆ¬å–) æˆ– 'local' (å¾æœ¬åœ° data è³‡æ–™å¤¾è®€å–)ã€‚é è¨­ç‚º 'api'ã€‚")
    args = parser.parse_args()

    # --- æ­¥é©Ÿ 1: æ ¹æ“šä¾†æºç²å–è³‡æ–™ ---
    if args.source == 'api':
        employees_data = fetch_and_save_from_api()
    else:  # args.source == 'local'
        employees_data = load_from_local()

    if not employees_data:
        print("æœªèƒ½ç²å–ä»»ä½•å“¡å·¥è³‡æ–™ï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
        return

    # --- æ­¥é©Ÿ 2: è½‰æ›è³‡æ–™æ ¼å¼ ---

    # å®šç¾©è³‡æ–™è¡¨ä¸­çš„æœ‰æ•ˆæ¬„ä½
    VALID_COLUMNS = {
        'emp_id',
        'c_name',
        'e_name',
        'dep_code',
        'job_status',
        'encrypt_emp_id',
        'per_seril_no',
        'encrypt_per_seril_no',
        'tit_name',
        'dep_name_act',
        'ofc_ext',
        'introduction',
        'cmp_ent_dte',
        'lev_exp_sdate',
        'user_id',
        'is_show_private_data',
        'photo_type',
        'is_show_download_photo',
        'cmp_code',
        'created_at'
    }

    transformed_data = []
    for record in employees_data:
        # å…ˆè½‰æ›æ‰€æœ‰éµç‚º snake_case
        snake_case_record = {camel_to_snake(key): value for key, value in record.items()}

        # åªä¿ç•™è³‡æ–™è¡¨ä¸­å­˜åœ¨çš„æ¬„ä½
        filtered_record = {
            k: (v if v != "" else None)
            for k, v in snake_case_record.items()
            if k in VALID_COLUMNS
        }

        # ç‰¹æ®Šè™•ç†æ—¥æœŸæ¬„ä½
        if 'cmp_ent_dte' in filtered_record and filtered_record['cmp_ent_dte']:
            try:
                # å‡è¨­æ—¥æœŸæ ¼å¼ç‚º "YYYY-MM-DD"
                filtered_record['cmp_ent_dte'] = filtered_record['cmp_ent_dte'].split('T')[0]
            except:
                filtered_record['cmp_ent_dte'] = None

        transformed_data.append(filtered_record)

    print("ğŸ”„ è³‡æ–™æ ¼å¼è½‰æ›å®Œæˆ (camelCase -> snake_case)ã€‚")

    # --- æ­¥é©Ÿ 3: åˆå§‹åŒ– Supabase ---
    if not all([SUPABASE_URL, SUPABASE_KEY]):
        print("ğŸ”´ éŒ¯èª¤ï¼šè«‹æª¢æŸ¥ .env æª”æ¡ˆä¸­çš„ Supabase URL/KEY æ˜¯å¦å·²è¨­å®šã€‚")
        return

    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        print("âœ… Supabase å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
    except Exception as e:
        print(f"ğŸ”´ Supabase åˆå§‹åŒ–å¤±æ•—: {e}")
        return

    # --- æ­¥é©Ÿ 4: åŒæ­¥è³‡æ–™åˆ° Supabaseï¼ˆå¢é‡æ›´æ–° + é›¢è·æ¨™è¨˜ï¼‰---
    stats = sync_employees_to_supabase(supabase, transformed_data)

    # --- è¼¸å‡ºçµ±è¨ˆæ‘˜è¦ ---
    print("\n" + "="*50)
    print("ğŸ“ˆ åŒæ­¥å®Œæˆï¼çµ±è¨ˆæ‘˜è¦ï¼š")
    print("="*50)
    print(f"   âœ… æ–°å¢å“¡å·¥ï¼š{stats['new']} ç­†")
    print(f"   ğŸ”„ æ›´æ–°å“¡å·¥ï¼š{stats['updated']} ç­†")
    print(f"   ğŸ‘‹ é›¢è·å“¡å·¥ï¼š{stats['departed']} ç­†")
    print(f"   ğŸ“Š ç¸½è¨ˆè™•ç†ï¼š{stats['new'] + stats['updated'] + stats['departed']} ç­†")
    print("="*50 + "\n")


if __name__ == "__main__":
    main()